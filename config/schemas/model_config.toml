# Model Configuration Schema

[model]
name = { type = "str", required = true }
family = { type = "str", allowed = ["llama", "gpt4", "claude"], required = true }
size = { type = "str", allowed = ["7b", "13b", "70b"], required = true }
variant = { type = "str", allowed = ["base", "chat", "code"], default = "base" }
revision = { type = "str", default = "main" }
quantization = { type = "str", allowed = ["none", "int8", "int4"], default = "none" }

[resources]
min_gpu_memory = { type = "int", min = 8, default = 8 }
max_batch_size = { type = "int", min = 1, default = 32 }
max_sequence_length = { type = "int", min = 512, default = 32768 }
device_map = { type = "str", allowed = ["auto", "balanced", "sequential"], default = "auto" }
torch_dtype = { type = "str", allowed = ["float32", "float16", "bfloat16"], default = "float16" }

[optimization]
attention_implementation = { type = "str", allowed = ["default", "flash", "h2o"], default = "default" }
use_bettertransformer = { type = "bool", default = false }
use_compile = { type = "bool", default = false }
compile_mode = { type = "str", allowed = ["default", "reduce-overhead", "max-autotune"], default = "default" }

[h2o]
enabled = { type = "bool", default = false }
window_length = { type = "int", min = 128, default = 512 }
heavy_hitter_tokens = { type = "int", min = 32, default = 128 }
compression = { type = "bool", default = true }

[security]
trust_remote_code = { type = "bool", default = false }
use_auth_token = { type = "bool", default = false }
verify_downloads = { type = "bool", default = true }
allowed_model_sources = { type = "list", element_type = "str", default = ["huggingface.co"] } 