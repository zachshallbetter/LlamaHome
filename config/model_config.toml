# Default Model Configuration

[model]
name = "llama"
family = "llama"
size = "13b"
variant = "chat"
revision = "main"
quantization = "none"

[resources]
min_gpu_memory = 16
max_batch_size = 32
max_sequence_length = 32768
device_map = "auto"
torch_dtype = "float16"

[optimization]
attention_implementation = "h2o"
use_bettertransformer = true
use_compile = true
compile_mode = "reduce-overhead"

[h2o]
enabled = true
window_length = 512
heavy_hitter_tokens = 128
compression = true

[security]
trust_remote_code = false
use_auth_token = false
verify_downloads = true
allowed_model_sources = ["huggingface.co"]

[models."llama3.3"]
name = "Llama 3.3"
versions = ["7b", "13b", "70b"]
variants = ["base", "chat"]
requires_gpu = true
max_tokens = 32768
context_window = 8192
temperature_range = [0.1, 2.0]

[models.gpt4]
name = "GPT-4 Turbo"
requires_key = true
max_tokens = 128000
context_window = 128000
temperature_range = [0.0, 2.0]
env_vars = ["LLAMAHOME_GPT4_API_KEY", "LLAMAHOME_GPT4_ORG_ID"]

[models.claude]
name = "Claude 3"
requires_key = true
model_variants = ["opus", "sonnet", "haiku"]
temperature_range = [0.0, 1.0]
env_vars = ["LLAMAHOME_CLAUDE_API_KEY", "LLAMAHOME_CLAUDE_ORG_ID"]

[models."llama3.3".min_gpu_memory]
7b = 8
13b = 16
70b = 80

[models."llama3.3".h2o_config]
enabled = true
window_length = 512
heavy_hitter_tokens = 128

[models.gpt4.rate_limits]
tokens_per_minute = 180000
requests_per_minute = 500

[models.claude.max_tokens]
opus = 200000
sonnet = 150000
haiku = 100000

[models.claude.context_window]
opus = 200000
sonnet = 150000
haiku = 100000

[models.claude.rate_limits]
tokens_per_minute = 150000
requests_per_minute = 450
