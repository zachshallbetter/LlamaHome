# Distributed Training Configuration

# Distributed setup
distributed:
  # Basic settings
  backend: "nccl"  # Communication backend (nccl, gloo)
  init_method: "env://"  # Initialization method
  world_size: 1  # Total number of processes
  num_nodes: 1  # Number of nodes
  node_rank: 0  # Current node rank
  
  # Process settings
  rank: 0  # Process rank
  local_rank: 0  # Local process rank
  master_addr: "localhost"  # Master node address
  master_port: "29500"  # Communication port
  
  # Model settings
  sync_batch_norm: true  # Use synchronized batch normalization
  find_unused_parameters: false  # Find unused parameters in DDP
  gradient_as_bucket_view: true  # Use gradient bucket view
  static_graph: false  # Use static graph optimization

# Resource management
resources:
  # GPU settings
  gpu_memory_fraction: 0.95  # Maximum GPU memory fraction
  gpu_batch_size: 16  # Per-GPU batch size
  gpu_workers: 4  # Workers per GPU
  
  # CPU settings
  cpu_workers: 8  # Total CPU workers
  pin_memory: true  # Pin memory for faster data transfer
  
  # Memory settings
  prefetch_factor: 2  # Number of batches to prefetch
  max_memory: "90%"  # Maximum memory usage

# Training optimization
optimization:
  # Precision settings
  mixed_precision: true  # Use mixed precision training
  gradient_checkpointing: true  # Use gradient checkpointing
  
  # Gradient settings
  gradient_clipping: 1.0  # Maximum gradient norm
  gradient_accumulation: 4  # Gradient accumulation steps
  
  # Batch settings
  dynamic_batch_size: true  # Adjust batch size dynamically
  min_batch_size: 1  # Minimum batch size
  max_batch_size: 32  # Maximum batch size

# Communication
communication:
  # Network settings
  timeout: 1800  # Communication timeout (seconds)
  broadcast_buffers: true  # Broadcast buffers in DDP
  bucket_cap_mb: 25  # DDP bucket size (MB)
  
  # Synchronization
  sync_frequency: 1  # Parameter sync frequency
  barrier_timeout: 300  # Barrier timeout (seconds)
  find_unused_parameters_freq: 100  # Check frequency

# Monitoring
monitoring:
  # Metrics settings
  metrics_interval: 10  # Metrics collection interval
  save_interval: 100  # Checkpoint save interval
  
  # Resource tracking
  track_memory: true  # Track memory usage
  track_gradients: true  # Track gradient statistics
  track_communication: true  # Track communication stats
  
  # Visualization
  tensorboard: true  # Use TensorBoard
  plot_metrics: true  # Generate metric plots
  log_level: "INFO"  # Logging level

# Checkpointing
checkpointing:
  # Save settings
  save_frequency: 1000  # Steps between checkpoints
  keep_last: 5  # Number of checkpoints to keep
  save_optimizer: true  # Save optimizer state
  
  # Load settings
  load_optimizer: true  # Load optimizer state
  strict_loading: true  # Strict state dict loading
  resume_training: true  # Resume from checkpoint

# Error handling
error_handling:
  # Recovery settings
  max_retries: 3  # Maximum retry attempts
  retry_delay: 30  # Delay between retries (seconds)
  
  # Fallback settings
  fallback_to_cpu: true  # Fall back to CPU if GPU fails
  save_on_error: true  # Save checkpoint on error
  
  # Monitoring
  alert_on_error: true  # Send alerts on error
  log_stacktrace: true  # Log full stacktrace 